{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72223fb3",
   "metadata": {},
   "source": [
    "This is a script to verify that commonly used metrics for binary classification are not invariant to arbitrary class distributions. Suppose we are given a binary classifier which predicts class 0 perfectly (100% accuracy) and class 1 with 50% accuracy. Now consider two test sets: the first contains 1 sample of class 0 and 2 samples of class 1, the second includes 2 samples for both classes. Assuming the samples are i.i.d, the predicted labels/scores are given in the following cell. \n",
    "\n",
    "We evaluate 6 metrics: weighted F1, matthews_corrcoef ($\\phi$), cohen_kappa_score ($\\kappa$), average_precision_score (AUPRC) and our proposed balanced acc and balance F1, the result follows:\n",
    "\n",
    "\n",
    "| Metric | Dataset 1 | Dataset 2 | Invariant |\n",
    "| :----: | :-------: | :-------: | :-------: |\n",
    "| Weighted F1 | 0.6667 | 0.7333 | ❌ |\n",
    "| $\\phi$ | 0.5000 | 0.5774 | ❌  |\n",
    "| $\\kappa$ | 0.4000 | 0.5000 | ❌  |\n",
    "| AUPRC | 0.8333 | 0.7500 | ❌  |\n",
    "| Balanced Acc | 0.7500 | 0.7500 | ✅ |\n",
    "| Balanced F1 | 0.7333 | 0.7333 | ✅ |\n",
    "\n",
    "\n",
    "**Conclusion: only the proposed balanced metrics are invariant to different class distributions in datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7057fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 for dataset 1: 0.6667\n",
      "Weighted F1 for dataset 2: 0.7333\n",
      "phi coefficient for dataset 1: 0.5000\n",
      "phi coefficient for dataset 2: 0.5774\n",
      "cohen kappa for dataset 1: 0.4000\n",
      "cohen kappa for dataset 2: 0.5000\n",
      "AUPRC for dataset 1: 0.8333\n",
      "AUPRC for dataset 2: 0.7500\n",
      "Balanced Acc for dataset 1: 0.7500\n",
      "Balanced Acc for dataset 2: 0.7500\n",
      "Balanced F1 for dataset 1: 0.7333\n",
      "Balanced F1 for dataset 2: 0.7333\n"
     ]
    }
   ],
   "source": [
    "import script._init_paths\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score, average_precision_score, f1_score\n",
    "from lib.core.evaluate import balanced_f1, balanced_accuracy_score\n",
    "\n",
    "\n",
    "# Dataset 1\n",
    "gt1 = [0, 1, 1] # ground truth labels\n",
    "pred1 = [0, 0, 1] # predicted labels/scores\n",
    "\n",
    "# Dataset 2\n",
    "gt2 = [0, 0, 1, 1] # ground truth labels\n",
    "pred2 = [0, 0, 0, 1] # predicted labels/scores\n",
    "\n",
    "print('Weighted F1 for dataset 1: %.4f' %(f1_score(gt1, pred1, average=\"weighted\")))\n",
    "print('Weighted F1 for dataset 2: %.4f' %(f1_score(gt2, pred2, average=\"weighted\")))\n",
    "print('phi coefficient for dataset 1: %.4f' %(matthews_corrcoef(gt1, pred1)))\n",
    "print('phi coefficient for dataset 2: %.4f' %(matthews_corrcoef(gt2, pred2)))\n",
    "print('cohen kappa for dataset 1: %.4f' %(cohen_kappa_score(gt1, pred1)))\n",
    "print('cohen kappa for dataset 2: %.4f' %(cohen_kappa_score(gt2, pred2)))\n",
    "print('AUPRC for dataset 1: %.4f' %(average_precision_score(gt1, pred1)))\n",
    "print('AUPRC for dataset 2: %.4f' %(average_precision_score(gt2, pred2)))\n",
    "print('Balanced Acc for dataset 1: %.4f' %(balanced_accuracy_score(gt1, pred1)))\n",
    "print('Balanced Acc for dataset 2: %.4f' %(balanced_accuracy_score(gt2, pred2)))\n",
    "print('Balanced F1 for dataset 1: %.4f' %(balanced_f1(gt1, pred1)))\n",
    "print('Balanced F1 for dataset 2: %.4f' %(balanced_f1(gt2, pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae554047",
   "metadata": {},
   "source": [
    "As proven in Thm 1 of the paper, only the proposed balanced metrics are *invariant* to label distribution shift. Therefore, to retain consistency with the standard split (balanced test set), only these 2 metrics can be used on an *imbalanced* test set. As discussed in the paper, this enables a much larger test set for better quality and reliability of the evaluation statistics. We provide empirical evidence of this claim by running the following toy example.\n",
    "\n",
    "In the following cell, we consider a 3-way classification problem where there are only 10 samples for the tail class 2. We can construct an imbalanced test set with sample size [1000, 100, 10] for each class (used for the balanced metrics), while the maximum size of a balanced test set is [10, 10, 10] for each class (used for the rest non-invariant metrics). Suppose we are given a classifier that outputs logits/scores for each class following the probability table below:\n",
    "\n",
    "| Class | Dim 0 | Dim 1 | Dim 2 |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| 0     | Uniform(0, 3) | Uniform(0, 2) | Uniform(0, 1) | \n",
    "| 1     | Uniform(0, 2) | Uniform(0, 3) | Uniform(0, 2) | \n",
    "| 2     | Uniform(0, 1) | Uniform(0, 2) | Uniform(0, 3) | \n",
    "\n",
    "where Uniform(a, b) stands for an continuous uniform distribution on interval [a, b]. Given a random seed, we resample the model output from the probability table above and compute the balanced metrics and other metrics. We repeat the process for multiple random seeds and keep track of the **standard devision** of the metrics across all seeds:\n",
    "\n",
    "| Metric | # Seed=3 | # Seed=5 | # Seed=10 | # Seed=20 | # Seed=50 | # Seed=100 |\n",
    "| :----: | :----: | :----: | :----: | :----: | :----: | :----: | \n",
    "| Weighted F1 | 0.0322 | 0.0563 | 0.1036 |  0.0892 | 0.0760 | 0.0853 | \n",
    "| $\\phi$ | 0.0513 | 0.0800 |  0.1427 | 0.1281 | 0.1140 | 0.1275 | \n",
    "| $\\kappa$ | 0.0471 | 0.0800 |  0.1435 | 0.1281 | 0.1132 | 0.1269 | \n",
    "| AUPRC | 0.0369 | 0.0289 |  0.0625 | 0.0703 | 0.0696 | 0.0763 | \n",
    "| AUROC | 0.0295 | **0.0242** | 0.0539 | 0.0613 | 0.0602 | 0.0672 | \n",
    "| Balanced Acc (standard) | 0.0314 | 0.0533 |  0.0957 | 0.0854 | 0.0754 | 0.0846 | \n",
    "| Balanced F1 (standard) | 0.0322 | 0.0563 |  0.1036 | 0.0892 | 0.0760 | 0.0853 | \n",
    "| Balanced Acc (random) | 0.0310 | 0.0414 |  0.0512 | 0.0471 | 0.0522 | 0.0483 | \n",
    "| Balanced F1 (random) | **0.0286** | 0.0374 |  **0.0504** | **0.0463** | **0.0506** | **0.0468** | \n",
    "\n",
    "for each column, the metric with lowest standard deviation is **bolded**.\n",
    "\n",
    "**Conclusion: the proposed balanced metrics (especially balanced f1) achieve consistently lower variance/standard deviation thanks to their invariance to label distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680251db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted f1 standard deviation: 0.0853\n",
      "Phi coefficient standard deviation: 0.1275\n",
      "Cohen kappa standard deviation: 0.1269\n",
      "AUPRC standard deviation: 0.0763\n",
      "AUROC standard deviation: 0.0672\n",
      "Balanced Acc (standard) standard deviation: 0.0846\n",
      "Balanced F1 (standard) standard deviation: 0.0853\n",
      "Balanced Acc (random) standard deviation: 0.0483\n",
      "Balanced F1 (random) standard deviation: 0.0468\n"
     ]
    }
   ],
   "source": [
    "import script._init_paths\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score, average_precision_score, f1_score, average_precision_score, roc_auc_score\n",
    "from lib.core.evaluate import balanced_f1, balanced_accuracy_score\n",
    "\n",
    "# Dataset random generator\n",
    "def data_gen(seed=0, samples=[10, 10, 10]):\n",
    "    np.random.seed(seed)\n",
    "    sample0, sample1, sample2 = samples[0], samples[1], samples[2]\n",
    "    gt0 = np.zeros(sample0)\n",
    "    score0 = np.concatenate([np.random.uniform(0, 3, sample0).reshape(-1, 1),\n",
    "                             np.random.uniform(0, 2, sample0).reshape(-1, 1),\n",
    "                             np.random.uniform(0, 1, sample0).reshape(-1, 1)], axis=1)\n",
    "\n",
    "    gt1 = np.ones(sample1)\n",
    "    score1 = np.concatenate([np.random.uniform(0, 2, sample1).reshape(-1, 1),\n",
    "                             np.random.uniform(0, 3, sample1).reshape(-1, 1),\n",
    "                             np.random.uniform(0, 2, sample1).reshape(-1, 1)], axis=1)\n",
    "\n",
    "    gt2 = np.ones(sample2) * 2\n",
    "    score2 = np.concatenate([np.random.uniform(0, 1, sample2).reshape(-1, 1),\n",
    "                             np.random.uniform(0, 2, sample2).reshape(-1, 1),\n",
    "                             np.random.uniform(0, 3, sample2).reshape(-1, 1)], axis=1)\n",
    "    \n",
    "    gt = np.concatenate([gt0, gt1, gt2])\n",
    "    score = np.concatenate([score0, score1, score2])\n",
    "    softmax = np.exp(score)/np.sum(np.exp(score), axis=1).reshape(-1, 1)\n",
    "    pred = np.argmax(score, axis=1)\n",
    "    return gt.astype('int16'), softmax, pred\n",
    "\n",
    "# Convert ground truth labels to one-hot labels\n",
    "def one_hot(gt):\n",
    "    one_hot = np.zeros((gt.size, gt.max() + 1))\n",
    "    one_hot[np.arange(gt.size), gt] = 1\n",
    "    return one_hot\n",
    "\n",
    "w_f1 = []\n",
    "phi = []\n",
    "ck = []\n",
    "auprc = []\n",
    "auroc = []\n",
    "b_acc_rand = []\n",
    "b_f1_rand = []\n",
    "b_acc_stand = []\n",
    "b_f1_stand = []\n",
    "\n",
    "for seed in range(0, 100):\n",
    "    np.random.seed(seed)\n",
    "    # on imbalanced test set (i.e., random split)\n",
    "    gt, _, pred = data_gen(seed, samples=[1000, 100, 10])\n",
    "    b_acc_rand.append(balanced_accuracy_score(gt, pred))\n",
    "    b_f1_rand.append(balanced_f1(gt, pred))\n",
    "    \n",
    "    # on balanced test set (i.e., standard split)\n",
    "    gt, softmax, pred = data_gen(seed, samples=[10, 10, 10])\n",
    "    w_f1.append(f1_score(gt, pred, average=\"weighted\"))\n",
    "    phi.append(matthews_corrcoef(gt, pred))\n",
    "    ck.append(cohen_kappa_score(gt, pred))\n",
    "    auprc.append(average_precision_score(one_hot(gt), softmax))\n",
    "    auroc.append(roc_auc_score(one_hot(gt), softmax))\n",
    "    b_acc_stand.append(balanced_accuracy_score(gt, pred))\n",
    "    b_f1_stand.append(balanced_f1(gt, pred))    \n",
    "\n",
    "print('Weighted f1 standard deviation: %.4f' %(np.std(w_f1)))\n",
    "print('Phi coefficient standard deviation: %.4f' %(np.std(phi)))\n",
    "print('Cohen kappa standard deviation: %.4f' %(np.std(ck)))\n",
    "print('AUPRC standard deviation: %.4f' %(np.std(auprc)))      \n",
    "print('AUROC standard deviation: %.4f' %(np.std(auroc)))\n",
    "print('Balanced Acc (standard) standard deviation: %.4f' %(np.std(b_acc_stand)))\n",
    "print('Balanced F1 (standard) standard deviation: %.4f' %(np.std(b_f1_stand)))\n",
    "print('Balanced Acc (random) standard deviation: %.4f' %(np.std(b_acc_rand)))\n",
    "print('Balanced F1 (random) standard deviation: %.4f' %(np.std(b_f1_rand)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1778f8c",
   "metadata": {},
   "source": [
    "From a different view, we explore a key property of a metric, **p-value**. Formally, p-value is the probability of obtaining results at least as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is correct. Simply put, **a smaller p-value represents stronger evidence in favor of the alternative hypothesis**, that is, more discrimination between the baselines can be obtained.\n",
    "We calculate the p-values for metrics including Balanced Acc, Balanced F1, AUROC, AUPRC, $\\phi$ and $\\kappa$ on four datasets under the random split, given the estimates of standard deviation from 3 random seeds (also known as [Student's t-test](https://en.wikipedia.org/wiki/Student's_t-test)).\n",
    "\n",
    "As p-value is only defined between two samples, for the 11 baselines, we calculated p-values pairwise across all baselines. The arithmetic mean of p-values are reported below:\n",
    "\n",
    "| Metric | HIV | SBAP | DrugBank | USPTO-50K |\n",
    "| :----: | :----: | :----: | :----: |  :----: | \n",
    "| $\\phi$ | 0.3262 | 0.3448 |  0.3619 |    0.2960   |\n",
    "| $\\kappa$ | 0.3196 | 0.3761 |  0.3566 |   0.2649   |\n",
    "| AUPRC | 0.2782 | 0.2826 |  0.2859 |   0.2800   |\n",
    "| AUROC | 0.4580 | 0.4693 | 0.4181 |   0.4288   |\n",
    "| Balanced Acc | **0.2197** | **0.2561** |  **0.2973** | 0.2547   |\n",
    "| Balanced F1 | 0.2210 | 0.2979 |  0.3133 |   **0.2540** |\n",
    "\n",
    "for each column, the metric with lowest p-value is **bolded**.\n",
    "\n",
    "**Conclusion: the proposed balanced metrics achieve consistently lower p-values on real-world datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894f485",
   "metadata": {},
   "source": [
    "**The overall conclusion:**\n",
    "- Experimentally, here in this script, we demonstrate **3 clear advantages** of the proposed metrics:\n",
    "    - They are the *only metrics* that are invariant to the label distribution of test sets.\n",
    "    - Due to 1, they are the *only metrics* that can be tested *without loss of fairness on much larger, imbalanced test sets*, resulting in significantly lower variance/uncertainty.\n",
    "    - Due to 2, the lower variance/uncertainty means that when ranking different models, the proposed metrics provide better statistical significance and discriminative power, which is evident in our pairwise t-tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImDrug",
   "language": "python",
   "name": "imdrug"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
