{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0d0736",
   "metadata": {},
   "source": [
    "This is a script to verify that commonly used metrics for binary classification are not invariant to arbitrary class distributions. Suppose we are given a binary classifier which predicts class 0 perfectly (100% accuracy) and class 1 with 50% accuracy. Now consider two test sets: the first contains 1 sample of class 0 and 2 samples of class 1, the second includes 2 samples for both classes. Assuming the samples are i.i.d, the predicted labels/scores are given in the following cell. \n",
    "\n",
    "We evaluate 6 metrics: weighted F1, matthews_corrcoef ($\\phi$), cohen_kappa_score ($\\kappa$), average_precision_score (AUPRC) and our proposed balanced acc and balance F1, the result follows:\n",
    "\n",
    "\n",
    "| Metric | Dataset 1 | Dataset 2 | Invariant |\n",
    "| :----: | :-------: | :-------: | :-------: |\n",
    "| Weighted F1 | 0.6667 | 0.7333 | ❌ |\n",
    "| $\\phi$ | 0.5000 | 0.5774 | ❌  |\n",
    "| $\\kappa$ | 0.4000 | 0.5000 | ❌  |\n",
    "| AUPRC | 0.8333 | 0.7500 | ❌  |\n",
    "| Balanced Acc | 0.7500 | 0.7500 | ✅ |\n",
    "| Balanced F1 | 0.7333 | 0.7333 | ✅ |\n",
    "\n",
    "\n",
    "**Conclusion: only the proposed balanced metrics are invariant to different class distributions in datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7057fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 for dataset 1: 0.6667\n",
      "Weighted F1 for dataset 2: 0.7333\n",
      "phi coefficient for dataset 1: 0.5000\n",
      "phi coefficient for dataset 2: 0.5774\n",
      "cohen kappa for dataset 1: 0.4000\n",
      "cohen kappa for dataset 2: 0.5000\n",
      "AUPRC for dataset 1: 0.8333\n",
      "AUPRC for dataset 2: 0.7500\n",
      "Balanced Acc for dataset 1: 0.7500\n",
      "Balanced Acc for dataset 2: 0.7500\n",
      "Balanced F1 for dataset 1: 0.7333\n",
      "Balanced F1 for dataset 2: 0.7333\n"
     ]
    }
   ],
   "source": [
    "import script._init_paths\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score, average_precision_score, f1_score\n",
    "from lib.core.evaluate import balanced_f1, balanced_accuracy_score\n",
    "\n",
    "\n",
    "# Dataset 1\n",
    "gt1 = [0, 1, 1] # ground truth labels\n",
    "pred1 = [0, 0, 1] # predicted labels/scores\n",
    "\n",
    "# Dataset 2\n",
    "gt2 = [0, 0, 1, 1] # ground truth labels\n",
    "pred2 = [0, 0, 0, 1] # predicted labels/scores\n",
    "\n",
    "print('Weighted F1 for dataset 1: %.4f' %(f1_score(gt1, pred1, average=\"weighted\")))\n",
    "print('Weighted F1 for dataset 2: %.4f' %(f1_score(gt2, pred2, average=\"weighted\")))\n",
    "print('phi coefficient for dataset 1: %.4f' %(matthews_corrcoef(gt1, pred1)))\n",
    "print('phi coefficient for dataset 2: %.4f' %(matthews_corrcoef(gt2, pred2)))\n",
    "print('cohen kappa for dataset 1: %.4f' %(cohen_kappa_score(gt1, pred1)))\n",
    "print('cohen kappa for dataset 2: %.4f' %(cohen_kappa_score(gt2, pred2)))\n",
    "print('AUPRC for dataset 1: %.4f' %(average_precision_score(gt1, pred1)))\n",
    "print('AUPRC for dataset 2: %.4f' %(average_precision_score(gt2, pred2)))\n",
    "print('Balanced Acc for dataset 1: %.4f' %(balanced_accuracy_score(gt1, pred1)))\n",
    "print('Balanced Acc for dataset 2: %.4f' %(balanced_accuracy_score(gt2, pred2)))\n",
    "print('Balanced F1 for dataset 1: %.4f' %(balanced_f1(gt1, pred1)))\n",
    "print('Balanced F1 for dataset 2: %.4f' %(balanced_f1(gt2, pred2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImDrug",
   "language": "python",
   "name": "imdrug"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
